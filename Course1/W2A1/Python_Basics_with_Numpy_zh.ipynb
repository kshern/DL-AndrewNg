{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python基础与Numpy（可选作业）\n",
    "\n",
    "欢迎来到您的第一个任务。本练习为您介绍Python的简要介绍。即使您之前使用过Python，这也将帮助您熟悉我们需要的功能。\n",
    "\n",
    "**说明：**\n",
    "\n",
    "- 您将使用Python 3。\n",
    "\n",
    "- 除非明确告诉您这样做，否则避免使用for循环和while循环。\n",
    "\n",
    "- 编写完函数后，请运行它正下方的单元格，以检查您的结果是否正确。\n",
    "\n",
    "**完成此任务后，您将：**\n",
    "\n",
    "- 能够使用iPython笔记本\n",
    "\n",
    "- 能够使用numpy函数和numpy矩阵/向量操作\n",
    "\n",
    "- 理解“广播”的概念\n",
    "\n",
    "- 能够向量化代码\n",
    "\n",
    "让我们开始吧！"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [About iPython Notebooks](#0)\n",
    "    - [Exercise 1](#ex-1)\n",
    "- [1 - Building basic functions with numpy](#1)\n",
    "    - [1.1 - sigmoid function, np.exp()](#1-1)\n",
    "        - [Exercise 2 - basic_sigmoid](#ex-2)\n",
    "        - [Exercise 3 - sigmoid](#ex-3)\n",
    "    - [1.2 - Sigmoid Gradient](#1-2)\n",
    "        - [Exercise 4 - sigmoid_derivative](#ex-4)\n",
    "    - [1.3 - Reshaping arrays](#1-3)\n",
    "        - [Exercise 5 - image2vector](#ex-5)\n",
    "    - [1.4 - Normalizing rows](#1-4)\n",
    "        - [Exercise 6 - normalize_rows](#ex-6)\n",
    "        - [Exercise 7 - softmax](#ex-7)\n",
    "- [2 - Vectorization](#2)\n",
    "    - [2.1 Implement the L1 and L2 loss functions](#2-1)\n",
    "        - [Exercise 8 - L1](#ex-8)\n",
    "        - [Exercise 9 - L2](#ex-9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## 关于 iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks 是嵌入网页中的交互式编码环境。您将在本课程中使用iPython笔记本。您只需要在“＃your code here”注释之间编写代码。编写代码后，可以通过按“SHIFT”+“ENTER”或单击笔记本上方的“运行单元格”（用播放符号表示）来运行单元格。\n",
    "\n",
    "我们经常在注释中指定“(≈ X行代码)”以告诉您需要编写多少代码。这只是一个粗略的估计，所以如果您的代码更长或更短，不要感到难过。\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "\n",
    "### 练习 1\n",
    "\n",
    "在下面的单元格中将test设置为`“Hello World”`以打印“Hello World”，然后运行下面的两个单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53ecb05267db45908a4d1c3f727eeb1c",
     "grade": false,
     "grade_id": "cell-edef848c738402d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# (≈ 1 line of code)\n",
    "# test = \n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "test = \"Hello World\"\n",
    "print(test)\n",
    "\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Hello World\n"
     ]
    }
   ],
   "source": [
    "print (\"test: \" + test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "test: Hello World"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "<b>What you need to remember </b>:\n",
    "    \n",
    "- Run your cells using SHIFT+ENTER (or \"Run cell\")\n",
    "- Write code in the designated areas using Python 3 only\n",
    "- Do not modify the code outside of the designated areas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - 使用numpy构建基本函数 ##\n",
    "\n",
    "Numpy是Python中科学计算的主要包。它由一个大型社区维护（www.numpy.org）。在这个练习中，您将学习几个关键的numpy函数，如`np.exp`、`np.log`和`np.reshape`。您需要知道如何使用这些函数来完成未来的任务。\n",
    "\n",
    "<a name='1-1'></a>\n",
    "\n",
    "### 1.1 - sigmoid函数，np.exp() ###\n",
    "\n",
    "在使用`np.exp()`之前，您将使用`math.exp()`来实现sigmoid函数。然后，您将看到为什么`np.exp()`比`math.exp()`更可取。\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "\n",
    "### 练习2 - basic_sigmoid\n",
    "\n",
    "构建一个函数，它返回实数x的sigmoid值。使用`math.exp(x)`进行指数函数。\n",
    "\n",
    "**提醒**：\n",
    "\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$有时也称为逻辑函数。它是一个非线性函数，不仅用于机器学习（逻辑回归），还用于深度学习。\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n",
    "\n",
    "要引用属于特定软件包的函数，可以使用`package_name.function()`调用它。运行下面的代码以查看使用`math.exp()`的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cec1a2c77dcc9c6a59470e3daf70f45",
     "grade": false,
     "grade_id": "cell-7f38ddeceef22374",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from public_tests import *\n",
    "\n",
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    # (≈ 1 line of code)\n",
    "    # s = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = 1/(1+math.exp(-x))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "150772e06208b50e91305b4ecf1421d4",
     "grade": true,
     "grade_id": "cell-6a7680d0a31b818e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_sigmoid(1) = 0.7310585786300049\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(\"basic_sigmoid(1) = \" + str(basic_sigmoid(1)))\n",
    "\n",
    "basic_sigmoid_test(basic_sigmoid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，在深度学习中我们很少使用“math”库，因为函数的输入是实数。在深度学习中，我们主要使用矩阵和向量，这就是为什么numpy更有用的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31628\\3742524197.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# x becomes a python list object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbasic_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# you will see this give an error when you run it, because x is a vector.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31628\\1525097619.py\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# s =\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# YOUR CODE STARTS HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# YOUR CODE ENDS HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "\n",
    "x = [1, 2, 3] # x becomes a python list object\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，如果 $ x = (x_1, x_2, ..., x_n)$ 是一个行向量，那么 `np.exp(x)` 将对 x 的每个元素应用指数函数。输出将如下所示：`np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "t_x = np.array([1, 2, 3])\n",
    "print(np.exp(t_x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，如果x是一个向量，那么像$s = x + 3$或$s = \\frac{1}{x}$这样的Python操作将输出与x大小相同的向量s。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# example of vector operation\n",
    "t_x = np.array([1, 2, 3])\n",
    "print (t_x + 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何时候，如果您需要更多有关numpy函数的信息，我们鼓励您查看[官方文档](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html)。\n",
    "\n",
    "您还可以在笔记本中创建一个新单元格并编写`np.exp？`（例如）以快速访问文档。\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "\n",
    "### 练习3 - sigmoid\n",
    "\n",
    "使用numpy实现sigmoid函数。\n",
    "\n",
    "**说明**：现在，x可以是实数、向量或矩阵。我们在numpy中用来表示这些形状（向量、矩阵等）的数据结构称为numpy数组。您现在不需要了解更多。\n",
    "\n",
    "$$ \\text{对于} x \\in \\mathbb{R}^n \\text{，} sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "\n",
    "x_1 \\\\\n",
    "\n",
    "x_2 \\\\\n",
    "\n",
    "... \\\\\n",
    "\n",
    "x_n \\\\\n",
    "\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\n",
    "\\frac{1}{1+e^{-x_1}} \\\\\n",
    "\n",
    "\\frac{1}{1+e^{-x_2}} \\\\\n",
    "\n",
    "... \\\\\n",
    "\n",
    "\\frac{1}{1+e^{-x_n}} \\\\\n",
    "\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcab43fa930612b05f46fdd1421443db",
     "grade": false,
     "grade_id": "cell-4c5ca880d9cf9642",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # s = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d6c9b80614b72a798e6df324bf20051",
     "grade": true,
     "grade_id": "cell-215cfe583f712716",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(t_x) = [0.73105858 0.88079708 0.95257413]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Sigmoid梯度\n",
    "\n",
    "正如你在讲座中所见，你需要计算梯度以优化损失函数使用反向传播。让我们编写你的第一个梯度函数。\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "\n",
    "### 练习 4 - sigmoid_derivative\n",
    "\n",
    "实现函数sigmoid_grad()来计算相对于其输入x的sigmoid函数的梯度。公式如下：\n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "\n",
    "通常情况下，你可以分两步进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "345fa4e729d4a65fc75c7b67b571b053",
     "grade": false,
     "grade_id": "cell-3e66ce00e171b40b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 2 lines of code)\n",
    "    # s = \n",
    "    # ds = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = sigmoid(x)\n",
    "    ds = s*(1-s)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db2f9ff9a137194ea17617bb758e1897",
     "grade": true,
     "grade_id": "cell-1b027673871951a1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(t_x) = [0.19661193 0.10499359 0.04517666]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(t_x) = \" + str(sigmoid_derivative(t_x)))\n",
    "\n",
    "sigmoid_derivative_test(sigmoid_derivative)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - 重塑数组 ###\n",
    "\n",
    "在深度学习中使用的两个常见的 numpy 函数是 [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) 和 [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)。\n",
    "\n",
    "- X.shape 用于获取矩阵/向量 X 的形状（维度）。\n",
    "\n",
    "- X.reshape(...) 用于将 X 重塑为其他维度。\n",
    "\n",
    "例如，在计算机科学中，图像由形状为 $(length，height，depth=3)$ 的 3D 数组表示。然而，当您将图像作为算法的输入读取时，您将其转换为形状为 $(length*height*3，1)$ 的向量。换句话说，您将 3D 数组“展开”或重塑为 1D 向量。\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "\n",
    "### 练习 5 - image2vector\n",
    "\n",
    "实现 `image2vector()`，它接受形状为 (length，height，3) 的输入，并返回形状为 (length\\*height\\*3，1) 的向量。例如，如果您想将形状为 (a，b，c) 的数组 v 重塑为形状为 (a*b，c) 的向量，则可以执行以下操作：\n",
    "\n",
    "``` python\n",
    "\n",
    "v = v.reshape((v.shape[0] * v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "\n",
    "```\n",
    "\n",
    "- 请不要将图像的维度硬编码为常量。相反，请使用 `image.shape[0]` 等查找所需的数量。\n",
    "\n",
    "- 您可以使用 `v = v.reshape(-1, 1)`。只需确保您理解为什么它有效即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdcbf18137f7cfa2d6ca62c4cf5c9c5d",
     "grade": false,
     "grade_id": "cell-b68b7900fdd239cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION:image2vector\n",
    "\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # v =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "339250a81053c3773a053c91cc46fb41",
     "grade": true,
     "grade_id": "cell-3b78eb8b041424f7",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "t_image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(t_image)))\n",
    "\n",
    "image2vector_test(image2vector)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-4'></a>\n",
    "### 1.4 - 规范化行\n",
    "\n",
    "在机器学习和深度学习中，我们经常使用的一种技术是对数据进行规范化。这通常会导致更好的性能，因为规范化后梯度下降会更快地收敛。在这里，通过规范化，我们指的是将 x 更改为 $ \\frac{x}{\\| x\\|} $（将 x 的每行向量除以其范数）。\n",
    "\n",
    "例如，如果\n",
    "\n",
    "$$x = \\begin{bmatrix}\n",
    "\n",
    "0 & 3 & 4 \\\\\n",
    "\n",
    "2 & 6 & 4 \\\\\n",
    "\n",
    "\\end{bmatrix}\\tag{3}$$\n",
    "\n",
    "则\n",
    "\n",
    "$$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "\n",
    "5 \\\\\n",
    "\n",
    "\\sqrt{56} \\\\\n",
    "\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "\n",
    "和\n",
    "\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "\n",
    "0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "\n",
    "\\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "请注意，您可以除以不同大小的矩阵，它也可以正常工作：这称为广播，您将在第5部分中学习它。\n",
    "\n",
    "使用`keepdims=True`，结果将正确地广播到原始的 x。\n",
    "\n",
    "`axis=1` 表示您将按行方式获取范数。如果您需要按列方式获取范数，则需要设置 `axis=0`。\n",
    "\n",
    "numpy.linalg.norm 还有另一个参数 `ord`，我们在其中指定要执行的规范化类型（在下面的练习中，您将执行 2-norm）。要熟悉规范化类型，可以访问 [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "\n",
    "### 练习 6 - normalize_rows\n",
    "\n",
    "实现 normalizeRows() 函数以规范化矩阵的行。将此函数应用于输入矩阵 x 后，x 的每一行应为单位长度向量（即长度为 1）。\n",
    "\n",
    "**注意**：不要尝试使用 `x /= x_norm`。对于矩阵除法，numpy 必须广播 x_norm，而该操作符 `/=` 不支持广播操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc112a436b66b6526a5fbfe1e7822ba8",
     "grade": false,
     "grade_id": "cell-5a030834cece94f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: normalize_rows\n",
    "\n",
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    # x_norm =\n",
    "    # Divide x by its norm.\n",
    "    # x =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    x_norm = np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    x = x/x_norm\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c780d41666f3144b0d68804c2df2e21",
     "grade": true,
     "grade_id": "cell-0910101c4de92095",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 3, 4],\n",
    "              [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))\n",
    "\n",
    "normalizeRows_test(normalize_rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "\n",
    "在normalize_rows()函数中，您可以尝试打印x_norm和x的形状，然后重新运行评估。您会发现它们具有不同的形状。这是正常的，因为x_norm对x的每一行取范数。因此，x_norm具有相同数量的行，但只有1列。那么当您将x除以x_norm时，它是如何工作的呢？这就是所谓的广播，我们现在来谈谈它！"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-7'></a>\n",
    "### Exercise 7 - softmax\n",
    "实现一个使用numpy的softmax函数。当您的算法需要分类两个或更多类时，可以将softmax视为归一化函数。您将在这个专业的第二门课程中学习更多关于softmax的知识。\n",
    "\n",
    "**Instructions**:\n",
    "- $\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     }$\n",
    "\n",
    "\\begin{align*}\n",
    " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $\\text{对于一个矩阵 } x \\in \\mathbb{R}^{m\\times n} \\text{, }$  $x_{ij}$ 指的是 $x$ 中第 $i$ 行第 $j$ 列的元素，因此我们有：\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在课程的后面，您将看到“m”用于表示“训练样本的数量”，每个训练样本都位于矩阵的自己的列中。此外，每个特征都将位于其自己的行中（每行都具有相同特征的数据）。\n",
    "\n",
    "对于每个训练示例的所有特征都应执行softmax，因此softmax将在列上执行（一旦我们在本课程的后面切换到该表示方式）。\n",
    "\n",
    "但是，在这个编程实践中，我们只专注于熟悉Python，因此我们使用常见的数学符号$m \\times n$，\n",
    "\n",
    "其中$m$是行数，$n$是列数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5100054e6e6ea2c9a6343b43406cf909",
     "grade": false,
     "grade_id": "cell-f41746c0a00bd2fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    # x_exp = ...\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    # x_sum = ...\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    # s = ...\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp,axis=1, keepdims = True)\n",
    "    s = x_exp / x_sum\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e598fb22ddfec51bbdb6d08af1076cc5",
     "grade": true,
     "grade_id": "cell-6f8e1f025948128c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([[9, 2, 5, 0, 0],\n",
    "                [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(t_x)))\n",
    "\n",
    "softmax_test(softmax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- 如果您打印上面的x_exp、x_sum和s的形状，并重新运行评估单元格，您会发现x_sum的形状为(2,1)，而x_exp和s的形状为(2,5)。由于Python的广播机制，x_exp/x_sum有效。\n",
    "  \n",
    "恭喜！您现在已经对Python的NumPy有了相当好的理解，并实现了一些在深度学习中会使用到的有用函数。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "<b>What you need to remember:</b>\n",
    "    \n",
    "- np.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
    "- the sigmoid function and its gradient\n",
    "- image2vector is commonly used in deep learning\n",
    "- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. \n",
    "- numpy has efficient built-in functions\n",
    "- broadcasting is extremely useful"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Vectorization\n",
    "\n",
    "\n",
    "在深度学习中，您处理非常大的数据集。因此，非计算优化的函数可能会成为算法的巨大瓶颈，并导致模型运行需要很长时间。为了确保您的代码具有计算效率，您将使用矢量化。例如，尝试区分以下点积、外积、逐元素乘积的实现的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.0ms\n",
      "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- Computation time = 0.0ms\n",
      "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- Computation time = 0.0ms\n",
      "gdot = [17.56086406 18.05890511 11.59793316]\n",
      " ----- Computation time = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1), len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i] * x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j] * x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.0ms\n",
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.0ms\n",
      "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----- Computation time = 0.0ms\n",
      "gdot = [17.56086406 18.05890511 11.59793316]\n",
      " ----- Computation time = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如您可能已经注意到的那样，矢量化实现更加简洁高效。对于更大的矢量/矩阵，运行时间的差异甚至更大。请注意，`np.dot()`执行矩阵-矩阵或矩阵-向量乘法。这与`np.multiply()`和`*`运算符（在Matlab/Octave中等同于`.*`）不同，后者执行逐元素相乘。\n",
    "\n",
    "请注意，`np.dot()`执行矩阵-矩阵或矩阵-向量乘法。这与`np.multiply()`和`*`运算符不同（它相当于Matlab/Octave中的`.*`），后者执行逐元素乘法。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 Implement the L1 and L2 loss functions\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Exercise 8 - L1 \n",
    "实现 numpy 中 L1 损失的向量化版本。您可能会发现 abs(x)（x 的绝对值）函数很有用。\n",
    "\n",
    "**Reminder**:\n",
    "- 损失用于评估您模型的性能。您的损失越大，您的预测值 ($\\hat{y}$) 与真实值 ($y$) 的差异就越大。在深度学习中，您使用像梯度下降这样的优化算法来训练模型并使成本最小化。\n",
    "\n",
    "- L1 损失的定义如下：\n",
    "\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c496fe0b5fb6fe1580162305cf97387",
     "grade": false,
     "grade_id": "cell-410accbd4d9a1fc2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # loss = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    loss = np.sum(np.abs(y-yhat))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7435251c8f0959006b7034fd1eb9a2d3",
     "grade": true,
     "grade_id": "cell-44ac3b50c1fba7a0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))\n",
    "\n",
    "L1_test(L1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-9'></a>\n",
    "### Exercise 9 - L2\n",
    "实现numpy向量化的L2损失版本。有几种实现L2损失的方法，但您可能会发现函数np.dot()很有用。作为提醒，如果$x = [x_1，x_2，...，x_n]$，那么np.dot(x，x) = $\\sum_{j=0}^n x_j^{2}$。\n",
    "\n",
    "- L2损失定义为$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d806d7037061895561c70f6abb03380e",
     "grade": false,
     "grade_id": "cell-a2624d0db4d22322",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # loss = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # loss = np.sum((y-yhat)**2)\n",
    "    loss = np.dot(y-yhat,y-yhat)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef616282fe941f332052dbb8641e9aa8",
     "grade": true,
     "grade_id": "cell-e7809ad65b5fe0ab",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L2 = \" + str(L2(yhat, y)))\n",
    "\n",
    "L2_test(L2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this assignment. We hope that this little warm-up exercise helps you in the future assignments, which will be more exciting and interesting!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "<b>What to remember:</b>\n",
    "    \n",
    "- Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
    "- You have reviewed the L1 and L2 loss.\n",
    "- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
